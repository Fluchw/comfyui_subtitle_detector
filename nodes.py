#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ComfyUI Subtitle Detector Node - ä½¿ç”¨ RapidOCR è¿›è¡Œå­—å¹•æ£€æµ‹
é‡å†™ç‰ˆï¼šä½¿ç”¨ç»Ÿä¸€çš„ VideoOCR API
"""

import time
import torch
import numpy as np
import cv2

from .video_ocr import OCRMode, ModelType, EnhanceMode, BoxStyle
from .rapid_ocr_engine import RapidOCREngine
from .frame_renderer import FrameRenderer
from .utils import BoxInterpolator

# ComfyUI è¿›åº¦æ¡æ”¯æŒ
try:
    from comfy.utils import ProgressBar
    HAS_PROGRESS_BAR = True
except ImportError:
    HAS_PROGRESS_BAR = False


class SubtitleDetectorRapidOCR:
    """
    ComfyUI å­—å¹•æ£€æµ‹èŠ‚ç‚¹ - ä½¿ç”¨ RapidOCR

    æ¨èé»˜è®¤é…ç½®ï¼š
    - OCRæ¨¡å¼: DETECT_ONLY (ä»…æ£€æµ‹ï¼Œæœ€å¿«)
    - æ¨¡å‹ç±»å‹: MOBILE (é€Ÿåº¦å¿«)
    - å¢å¼ºæ¨¡å¼: SHARPEN (é”åŒ–)
    - ç½®ä¿¡åº¦é˜ˆå€¼: 0.3 (æ£€æµ‹æ¨¡å¼æ¨è)
    - ç¼©æ”¾å› å­: 0.8 (é™ä½åˆ†è¾¨ç‡æå‡é€Ÿåº¦)
    - æ ‡æ³¨æ ·å¼: RED_HOLLOW (çº¢è‰²ç©ºå¿ƒæ¡†)
    """

    def __init__(self):
        pass

    @classmethod
    def INPUT_TYPES(cls):
        return {
            "required": {
                # è¾“å…¥è§†é¢‘æµ
                "images": ("IMAGE",),

                # ===== OCR æ¨¡å¼ =====
                "ocr_mode": (["detect_only", "detect_rec", "full"], {"default": "detect_only"}),

                # ===== æ¨¡å‹ç±»å‹ =====
                "model_type": (["MOBILE", "SERVER"], {"default": "MOBILE"}),

                # ===== ç½®ä¿¡åº¦é˜ˆå€¼ =====
                "confidence_threshold": ("FLOAT", {"default": 0.3, "min": 0.1, "max": 1.0, "step": 0.05}),

                # ===== ç¼©æ”¾å› å­ =====
                "scale_factor": ("FLOAT", {"default": 0.8, "min": 0.1, "max": 2.0, "step": 0.1}),

                # ===== å›¾åƒå¢å¼º =====
                "enhance_mode": (["sharpen", "clahe", "binary", "both", "denoise", "denoise_sharpen", "None"], {"default": "sharpen"}),

                # ===== æ ‡æ³¨æ ·å¼ =====
                "box_style": (["red_hollow", "green_fill", "mask"], {"default": "red_hollow"}),

                # ===== è·³å¸§è®¾ç½® =====
                "skip_frames": ("INT", {"default": 0, "min": 0, "max": 10}),

                # ===== GPU åŠ é€Ÿ =====
                "use_cuda": ("BOOLEAN", {"default": True}),

                # ===== æ’å€¼æ¨¡å¼ =====
                "interpolate_mode": (["union", "linear"], {"default": "union"}),
            },
            "optional": {
                # è¯†åˆ«æ‰¹å¤„ç†å¤§å° (ä»…åœ¨è¯†åˆ«æ¨¡å¼ä¸‹ç”Ÿæ•ˆ)
                "rec_batch_num": ("INT", {"default": 6, "min": 1, "max": 16}),
            }
        }

    RETURN_TYPES = ("IMAGE", "MASK")
    RETURN_NAMES = ("annotated_images", "subtitle_mask")
    FUNCTION = "process_images"
    CATEGORY = "SubtitleDetector"

    def process_images(self, images, ocr_mode, model_type, confidence_threshold, scale_factor,
                      enhance_mode, box_style, skip_frames, use_cuda, interpolate_mode,
                      rec_batch_num=6):


        # è½¬æ¢æšä¸¾å€¼
        enhance_mode_val = None if enhance_mode == "None" else enhance_mode

        print(f"SubtitleDetector (RapidOCR) - é…ç½®:")
        print(f"  OCRæ¨¡å¼: {ocr_mode}")
        print(f"  æ¨¡å‹ç±»å‹: {model_type}")
        print(f"  ç½®ä¿¡åº¦é˜ˆå€¼: {confidence_threshold}")
        print(f"  ç¼©æ”¾å› å­: {scale_factor}")
        print(f"  å¢å¼ºæ¨¡å¼: {enhance_mode_val}")
        print(f"  æ ‡æ³¨æ ·å¼: {box_style}")
        print(f"  è·³å¸§: {skip_frames}")
        print(f"  GPUåŠ é€Ÿ: {use_cuda}")

        # æ ¹æ® OCR æ¨¡å¼è®¾ç½®å‚æ•°
        if ocr_mode == "detect_only":
            use_det, use_cls, use_rec = True, False, False
            detect_only = True
        elif ocr_mode == "detect_rec":
            use_det, use_cls, use_rec = True, False, True
            detect_only = False
        elif ocr_mode == "full":
            use_det, use_cls, use_rec = True, True, True
            detect_only = False
        else:
            use_det, use_cls, use_rec = True, False, False
            detect_only = True

        # åˆå§‹åŒ– RapidOCR å¼•æ“
        ocr_engine = RapidOCREngine(
            detect_only=detect_only,
            confidence_threshold=confidence_threshold,
            scale_factor=scale_factor,
            enhance_mode=enhance_mode_val,
            use_det=use_det,
            use_cls=use_cls,
            use_rec=use_rec,
            det_limit_side_len=960,
            det_limit_type="max",
            use_cuda=use_cuda,
            rec_batch_num=rec_batch_num,
            model_type=model_type
        )

        # åˆå§‹åŒ–æ¸²æŸ“å™¨
        renderer = FrameRenderer(box_style=box_style)

        # åˆå§‹åŒ–æ’å€¼å™¨
        interpolator = BoxInterpolator(interpolate_mode=interpolate_mode)

        # ComfyUI è¾“å…¥æ ¼å¼: Tensor [Batch, H, W, C], range 0-1
        batch_size = len(images)
        h, w = images.shape[1], images.shape[2]
        input_device = images.device

        # æµå¼å¤„ç†ï¼šé¢„åˆ†é…è¾“å‡º tensorï¼ˆåœ¨ CPU ä¸Šé¿å…å†…å­˜æº¢å‡ºï¼‰
        print(f"\nSubtitleDetector: æµå¼å¤„ç† {batch_size} å¸§...")
        output_images_tensor = torch.zeros((batch_size, h, w, 3), dtype=torch.float32)
        output_masks_tensor = torch.zeros((batch_size, h, w), dtype=torch.float32)

        # è¾…åŠ©å‡½æ•°
        def tensor_to_cv2(img_tensor):
            """RGB float -> BGR uint8"""
            img_np = (img_tensor.cpu().numpy() * 255).clip(0, 255).astype(np.uint8)
            return cv2.cvtColor(img_np, cv2.COLOR_RGB2BGR)

        def cv2_to_numpy(img_cv2):
            """BGR uint8 -> RGB float numpy"""
            img_rgb = cv2.cvtColor(img_cv2, cv2.COLOR_BGR2RGB)
            return img_rgb.astype(np.float32) / 255.0

        # å¼€å§‹å¤„ç†
        start_time = time.time()
        last_log_time = start_time

        # åˆå§‹åŒ–è¿›åº¦æ¡
        pbar = ProgressBar(batch_size) if HAS_PROGRESS_BAR else None

        buffer = []
        frame_id = 0
        output_idx = 0
        last_boxes = []

        for i in range(batch_size):
            # é€å¸§å–å‡ºå¹¶ç«‹å³è½¬æ¢ï¼Œå‡å°‘å†…å­˜å ç”¨
            curr_tensor = images[i]
            frame = tensor_to_cv2(curr_tensor)

            # è·³å¸§é€»è¾‘
            should_process = (skip_frames == 0) or (frame_id % (skip_frames + 1) == 0)

            if should_process:
                boxes, detections = ocr_engine.process_frame(frame)
                last_boxes = list(boxes)
                buffer.append({
                    "frame": frame,
                    "boxes": list(boxes),
                    "detections": list(detections),
                    "skipped": False
                })
            else:
                # è·³è¿‡çš„å¸§ä½¿ç”¨ä¸Šä¸€å¸§çš„ç»“æœ
                buffer.append({
                    "frame": frame,
                    "boxes": list(last_boxes),
                    "detections": [],
                    "skipped": True
                })

            # 3 å¸§æ»‘åŠ¨çª—å£æ’å€¼
            if len(buffer) >= 3:
                prev_data = buffer[0]
                curr_data = buffer[1]
                next_data = buffer[2]

                is_skipped_frame = curr_data.get("skipped", False)

                if skip_frames > 0 and is_skipped_frame:
                    if prev_data["boxes"] and next_data["boxes"]:
                        new_boxes, new_detections, was_interp = interpolator.interpolate_frame(
                            prev_data["boxes"], curr_data["boxes"], next_data["boxes"],
                            prev_data["detections"], curr_data["detections"], next_data["detections"]
                        )
                        curr_data["boxes"] = new_boxes
                        curr_data["detections"] = new_detections
                else:
                    new_boxes, new_detections, was_interp = interpolator.interpolate_frame(
                        prev_data["boxes"], curr_data["boxes"], next_data["boxes"],
                        prev_data["detections"], curr_data["detections"], next_data["detections"]
                    )
                    if was_interp:
                        curr_data["boxes"] = new_boxes
                        curr_data["detections"] = new_detections

                # æ¸²æŸ“å¹¶ç›´æ¥å†™å…¥é¢„åˆ†é…çš„ tensor
                annotated_frame, mask_frame = renderer.draw_boxes(prev_data["frame"], prev_data["boxes"])
                output_images_tensor[output_idx] = torch.from_numpy(cv2_to_numpy(annotated_frame))

                if mask_frame is not None:
                    if len(mask_frame.shape) == 3:
                        mask_gray = cv2.cvtColor(mask_frame, cv2.COLOR_BGR2GRAY)
                    else:
                        mask_gray = mask_frame
                    output_masks_tensor[output_idx] = torch.from_numpy(mask_gray.astype(np.float32) / 255.0)

                output_idx += 1
                buffer.pop(0)

            frame_id += 1

            # æ›´æ–°è¿›åº¦æ¡
            if pbar is not None:
                pbar.update(1)

            # æ¯ç§’è¾“å‡ºä¸€æ¬¡è¿›åº¦
            current_time = time.time()
            if current_time - last_log_time >= 1.0 or i == batch_size - 1:
                elapsed = current_time - start_time
                fps = (i + 1) / elapsed if elapsed > 0 else 0
                percent = (i + 1) / batch_size * 100
                eta = (batch_size - i - 1) / fps if fps > 0 else 0
                print(f"è¿›åº¦: {i+1}/{batch_size} ({percent:.1f}%) | FPS: {fps:.2f} | é¢„è®¡å‰©ä½™: {eta:.1f}s")
                last_log_time = current_time

        # æ¸…ç©ºç¼“å†²åŒº
        for data in buffer:
            annotated_frame, mask_frame = renderer.draw_boxes(data["frame"], data["boxes"])
            output_images_tensor[output_idx] = torch.from_numpy(cv2_to_numpy(annotated_frame))

            if mask_frame is not None:
                if len(mask_frame.shape) == 3:
                    mask_gray = cv2.cvtColor(mask_frame, cv2.COLOR_BGR2GRAY)
                else:
                    mask_gray = mask_frame
                output_masks_tensor[output_idx] = torch.from_numpy(mask_gray.astype(np.float32) / 255.0)

            output_idx += 1

        # å¦‚æœåŸè¾“å…¥åœ¨ GPU ä¸Šï¼Œå°†ç»“æœç§»å› GPU
        if input_device.type != 'cpu':
            output_images_tensor = output_images_tensor.to(input_device)
            output_masks_tensor = output_masks_tensor.to(input_device)

        total_time = time.time() - start_time
        avg_fps = batch_size / total_time if total_time > 0 else 0

        print(f"\nâœ… å¤„ç†å®Œæˆï¼")
        print(f"  æ€»è€—æ—¶: {total_time:.2f}s")
        print(f"  å¹³å‡ FPS: {avg_fps:.2f}")

        return (output_images_tensor, output_masks_tensor)


# ComfyUI èŠ‚ç‚¹æ³¨å†Œ
NODE_CLASS_MAPPINGS = {
    "SubtitleDetectorRapidOCR": SubtitleDetectorRapidOCR
}

NODE_DISPLAY_NAME_MAPPINGS = {
    "SubtitleDetectorRapidOCR": "ğŸ¥ Subtitle Detector (RapidOCR v3)"
}
